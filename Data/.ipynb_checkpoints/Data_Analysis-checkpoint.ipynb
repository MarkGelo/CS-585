{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv while replacing nan as missing\n",
    "reddit = {'reddit_stance_0': pd.read_csv('reddit_stance/reddit_stance_0.csv').fillna('Missing')}\n",
    "twitter = {'twitter_stance_0': pd.read_csv('twitter_stance/twitter_stance_0.csv').fillna('Missing'),\n",
    "          'twitter_stance_1': pd.read_csv('twitter_stance/twitter_stance_1.csv').fillna('Missing'),\n",
    "          'twitter_stance_2': pd.read_csv('twitter_stance/twitter_stance_2.csv').fillna('Missing'),\n",
    "          'twitter_stance_3': pd.read_csv('twitter_stance/twitter_stance_3.csv').fillna('Missing'),\n",
    "          'twitter_stance_4': pd.read_csv('twitter_stance/twitter_stance_4.csv').fillna('Missing'),}\n",
    "\n",
    "# twitter_stance_3 has \"unclear,unclear\" as a label\n",
    "# twitter_stance_4 has \"anti-mitigation,anti-mitigation\"\n",
    "# reddit_stance_0 has \"pro-mitigation,promitigation\" as one of the label\n",
    "# 2 labels in one, but 1 always missing, so prob just parsing error\n",
    "# thus, move the second label to the right, updating other labels as necessary\n",
    "\n",
    "for k, v in twitter.items():\n",
    "    for anno in v.iloc[:, 1:]:\n",
    "        col_idx = v.columns.get_loc(anno)\n",
    "        if not v.loc[v[anno].str.contains(','), anno].empty: # has \"unclear,unclear\" etc, 2 labels in one cell\n",
    "            #print(v.loc[v[anno].str.contains(','), :])\n",
    "            idx = v.index[v[anno].str.contains(',')]\n",
    "            cur_label = v.loc[idx, anno].values.item()\n",
    "            labels = cur_label.split(',')\n",
    "            \n",
    "            # move labels to the right, distributing them, so one annotator would have one label\n",
    "            # current annotator would have first label\n",
    "            v.iloc[idx, col_idx] = labels.pop(0)\n",
    "            #print(v.iloc[idx, col_idx])\n",
    "            for z in range(col_idx + 1, len(v.columns)):\n",
    "                if labels and v.iloc[idx, z].values.item() == 'Missing':  \n",
    "                    v.iloc[idx, z] = labels.pop(0)\n",
    "                else: # replace and move to right\n",
    "                    labels.append(v.iloc[idx, z])\n",
    "                    v.iloc[idx, z] = labels.pop(0)\n",
    "                #print(v.iloc[idx, z])\n",
    "# Thus changed line 149 in twitter_stance_3 from \"unclear,unclear\" \"unclear\" \"pro-mitigation\" \"\"\n",
    "# to \"unclear\" \"unclear\" \"unclear\" \"pro-mitigation\"\n",
    "# line 301 in twitter_stance_4 from \"anti-mitigation,antimitigation\" \"unclear\" \"\" \"pro-mitigation\"\n",
    "# to \"anti-mitigation\" \"anti-mitigation\" \"unclear\" \"pro-mitigation\"\n",
    "            \n",
    "for k, v in reddit.items():\n",
    "    for anno in v.iloc[:, 1:]:\n",
    "        col_idx = v.columns.get_loc(anno)\n",
    "        if not v.loc[v[anno].str.contains(','), anno].empty: # has \"unclear,unclear\" etc, 2 labels in one cell\n",
    "            #print(v.loc[v[anno].str.contains(','), :])\n",
    "            idx = v.index[v[anno].str.contains(',')]\n",
    "            cur_label = v.loc[idx, anno].values.item()\n",
    "            labels = cur_label.split(',')\n",
    "            \n",
    "            # move labels to the right, distributing them, so one annotator would have one label\n",
    "            # current annotator would have first label\n",
    "            v.iloc[idx, col_idx] = labels.pop(0)\n",
    "            #print(v.iloc[idx, col_idx])\n",
    "            for z in range(col_idx + 1, len(v.columns)):\n",
    "                if labels and v.iloc[idx, z].values.item() == 'Missing':  \n",
    "                    v.iloc[idx, z] = labels.pop(0)\n",
    "                else: # replace and move to right\n",
    "                    labels.append(v.iloc[idx, z])\n",
    "                    v.iloc[idx, z] = labels.pop(0)\n",
    "                #print(v.iloc[idx, z])\n",
    "# line 101 in reddit_stance_0 from \"pro-mitigation,pro-mitigation\" \"pro-mitigation\" \"pro-mitigation\" \"\"\n",
    "# to \"pro-mitigation\" \"pro-mitigation\" \"pro-mitigation\" \"pro-mitigation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter_stance_0\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_32', 'annotation_33'): 0.2534050781906845\n",
      "('annotation_32', 'annotation_35'): 0.40854106796765366\n",
      "('annotation_32', 'annotation_17'): 0.23917483660130712\n",
      "('annotation_33', 'annotation_35'): 0.32735426008968616\n",
      "('annotation_33', 'annotation_17'): 0.3911261152640463\n",
      "('annotation_35', 'annotation_17'): 0.4136460554371002\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_32: 0.30037366091988177\n",
      "annotation_33: 0.323961817848139\n",
      "annotation_35: 0.3831804611648133\n",
      "annotation_17: 0.3479823357674845\n",
      "\n",
      "\n",
      "twitter_stance_1\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_26', 'annotation_37'): 0.14795244385733164\n",
      "('annotation_26', 'annotation_38'): 0.22745024472602615\n",
      "('annotation_26', 'annotation_39'): 0.16167664670658677\n",
      "('annotation_37', 'annotation_38'): 0.3147601660581204\n",
      "('annotation_37', 'annotation_39'): 0.44672131147540983\n",
      "('annotation_38', 'annotation_39'): 0.2834474695172874\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_26: 0.1790264450966482\n",
      "annotation_37: 0.3031446404636206\n",
      "annotation_38: 0.2752192934338113\n",
      "annotation_39: 0.29728180923309466\n",
      "\n",
      "\n",
      "twitter_stance_2\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_113', 'annotation_114'): 0.23612454070779343\n",
      "('annotation_113', 'annotation_115'): 0.2319587628865979\n",
      "('annotation_113', 'annotation_116'): 0.24873096446700504\n",
      "('annotation_114', 'annotation_115'): 0.16709511568123392\n",
      "('annotation_114', 'annotation_116'): 0.2042306723747167\n",
      "('annotation_115', 'annotation_116'): 0.27222222222222225\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_113: 0.2389380893537988\n",
      "annotation_114: 0.202483442921248\n",
      "annotation_115: 0.22375870026335135\n",
      "annotation_116: 0.24172795302131467\n",
      "\n",
      "\n",
      "twitter_stance_3\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_97', 'annotation_98'): 0.10136592379583043\n",
      "('annotation_97', 'annotation_99'): 0.12733883796682066\n",
      "('annotation_97', 'annotation_100'): 0.21534879011845698\n",
      "('annotation_98', 'annotation_99'): 0.09672563041023707\n",
      "('annotation_98', 'annotation_100'): 0.2951127819548872\n",
      "('annotation_99', 'annotation_100'): 0.16747956540751452\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_97: 0.148017850627036\n",
      "annotation_98: 0.1644014453869849\n",
      "annotation_99: 0.13051467792819074\n",
      "annotation_100: 0.22598037916028624\n",
      "\n",
      "\n",
      "twitter_stance_4\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_81', 'annotation_82'): 0.2404394443099006\n",
      "('annotation_81', 'annotation_83'): 0.3133135184995387\n",
      "('annotation_81', 'annotation_84'): 0.25992374971966803\n",
      "('annotation_82', 'annotation_83'): 0.19971813972216634\n",
      "('annotation_82', 'annotation_84'): 0.3793103448275861\n",
      "('annotation_83', 'annotation_84'): 0.20124481327800825\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_81: 0.2712255708430358\n",
      "annotation_82: 0.273155976286551\n",
      "annotation_83: 0.2380921571665711\n",
      "annotation_84: 0.28015963594175414\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitter_annotators = {}\n",
    "\n",
    "for k, v in twitter.items():\n",
    "    current_twitter = v.iloc[:, 1:]\n",
    "    \n",
    "    # get labels for each annotator\n",
    "    annotators = {}\n",
    "    for anno in current_twitter:\n",
    "        annotators[anno] = current_twitter[anno].values\n",
    "    \n",
    "    # get each pair of annotators -- 1,3 and 3,1 are same\n",
    "    # calculate agreement score\n",
    "    scores = {}\n",
    "    pairs = list(combinations(annotators, 2))\n",
    "    for pair in pairs:\n",
    "        scores[pair] = cohen_kappa_score(annotators[pair[0]], annotators[pair[1]])\n",
    "        \n",
    "    # claculate average kappa score\n",
    "    annotators_avg = {}\n",
    "    for k_, v_ in annotators.items():\n",
    "        #print(k_)\n",
    "        total = 0\n",
    "        for k__, v__ in scores.items():\n",
    "            if k_ in k__: # find pairs where current annotator is in -- 3 pairs\n",
    "                #print(k__)\n",
    "                total += scores[k__]\n",
    "        annotators_avg[k_] = total / 3\n",
    "    twitter_annotators[k] = annotators_avg\n",
    "        \n",
    "    print(k)\n",
    "    print(\"Cohen's kappa agreement score:\")\n",
    "    print('\\n'.join([str(k_) + \": \" + str(v_) for k_, v_ in scores.items()]))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Average Kappa Score:')\n",
    "    print('\\n'.join([str(k_) + \": \" + str(v_) for k_, v_ in annotators_avg.items()]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assemble dataset\n",
    "final_data = {'text': [], 'label': []}\n",
    "# for dupe texts -- \"Looks like #CovidVaccine will ...\", \"STUDIES SHOW MORPHINE MILLIGRAM ...\", etc\n",
    "unique_texts = {}\n",
    "dupes = {}\n",
    "for k, v in twitter.items():\n",
    "    cur_data = v\n",
    "    annotators = twitter_annotators[k]\n",
    "    \n",
    "    # remove annotators with kappa < 0.2 -- unreliable\n",
    "    for anno in annotators:\n",
    "        if annotators[anno] < 0.2:\n",
    "            cur_data = cur_data.drop(columns = [anno])\n",
    "            #print('unreliable', anno)\n",
    "    \n",
    "    # get frequent labels for each text\n",
    "    frequent_labels = cur_data.iloc[:, 1:].mode(axis = 1)\n",
    "    #print(frequent_labels)\n",
    "    for i in range(len(cur_data)):\n",
    "        cur_text = cur_data.loc[i, 'text']\n",
    "        \n",
    "        freqs = {key: (0, 0) for key in frequent_labels.loc[i].tolist() if not pd.isnull(key)}\n",
    "        # key: (total kappa, # annotators)\n",
    "        # use freq label with higher reliability annotators\n",
    "        \n",
    "        # get kappas for the annotators that chose the most frequent for current text\n",
    "        for anno, cur_label in cur_data.iloc[i, 1:].iteritems():\n",
    "            #print('index: ', anno, 'value: ', cur_label)\n",
    "            if cur_label in freqs:\n",
    "                freqs[cur_label] = (freqs[cur_label][0] + annotators[anno], freqs[cur_label][1] + 1)\n",
    "        \n",
    "        # get best label based on reliability\n",
    "        max_avg_kappa = 0\n",
    "        best_label = 'None'\n",
    "        for cur_label, cur_v in freqs.items():\n",
    "            cur_avg_kappa = cur_v[0] / cur_v[1] if cur_v[1] else 0\n",
    "            if cur_avg_kappa > max_avg_kappa and cur_label != 'Missing': \n",
    "                # if missing ignore, so essentially use the next available highest kappa\n",
    "                max_avg_kappa = cur_avg_kappa\n",
    "                best_label = cur_label\n",
    "                \n",
    "        # if text is a duplicate, keep highest kappa\n",
    "        if cur_text in unique_texts:\n",
    "            if cur_text in dupes: # already ahve one duplicate -- if current lable higher kappa then change\n",
    "                if max_avg_kappa > dupes[cur_text][1]:\n",
    "                    dupes[cur_text] = (best_label, max_avg_kappa)\n",
    "            else: # new duplkicate, use last dupe (in unique texts) and choose higher kappa\n",
    "                if max_avg_kappa > unique_texts[cur_text][1]: # current label is best, so change it\n",
    "                    dupes[cur_text] = (best_label, max_avg_kappa)\n",
    "                else: # keep value in unique text\n",
    "                    dupes[cur_text] = (unique_texts[cur_text][0], unique_texts[cur_text][1])\n",
    "        else:\n",
    "            unique_texts[cur_text] = (best_label, max_avg_kappa)\n",
    "        \n",
    "        final_data['text'].append(cur_text)\n",
    "        final_data['label'].append(best_label)\n",
    "    \n",
    "final_dataset = pd.DataFrame(data = final_data)\n",
    "# remove duplicates and update the label with the highest kappa\n",
    "# 11 dupes with 4 uniques, so remove 7\n",
    "final_dataset = final_dataset.drop_duplicates(subset = ['text'])\n",
    "for k in dupes: # update label for the dupes to the most frequent/kappa\n",
    "    final_dataset.loc[final_dataset['text'] == k, 'label'] = dupes[k][0]\n",
    "\n",
    "# Remove None labels -- labels that were 'Missing' even after choosing the most frequent and avg kappa scores\n",
    "# 2 removed from twitter_stance_3\n",
    "# twitter_stance_3 only had 1 reliable annotator (avg kappa > 0.2) so this was the annotator missing labels\n",
    "FINAL = final_dataset[final_dataset['label'] != 'None']\n",
    "FINAL.to_csv('PRIMARY-Twitter_Stance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Follow the CDC guidelines. Don’t become a stat...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you agree with CDC guidelines that children...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, both #Pharmaceutical companies #lilly and ...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The CDC's guidelines are clear; you just don't...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CDC Updates School Guidelines For Students Ret...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>.Sprint To Develop A #COVIDVaccine – // https:...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Here in the U.S. some localities have brought ...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>Sanitizer &amp;amp; Mask Manufacturers After Russi...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>We are following all CDC guidelines through a ...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>People be losing hopes on covid vaccine like i...</td>\n",
       "      <td>anti-mitigation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1491 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text            label\n",
       "0     Follow the CDC guidelines. Don’t become a stat...   pro-mitigation\n",
       "1     Do you agree with CDC guidelines that children...          unclear\n",
       "2     So, both #Pharmaceutical companies #lilly and ...          unclear\n",
       "3     The CDC's guidelines are clear; you just don't...   pro-mitigation\n",
       "4     CDC Updates School Guidelines For Students Ret...          unclear\n",
       "...                                                 ...              ...\n",
       "1495  .Sprint To Develop A #COVIDVaccine – // https:...          unclear\n",
       "1496  Here in the U.S. some localities have brought ...   pro-mitigation\n",
       "1497  Sanitizer &amp; Mask Manufacturers After Russi...          unclear\n",
       "1498  We are following all CDC guidelines through a ...   pro-mitigation\n",
       "1499  People be losing hopes on covid vaccine like i...  anti-mitigation\n",
       "\n",
       "[1491 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_stance_0\n",
      "Cohen's kappa agreement score:\n",
      "('annotation_24', 'annotation_10'): 0.45949087527606647\n",
      "('annotation_24', 'annotation_12'): 0.525124027565971\n",
      "('annotation_24', 'annotation_71'): 0.40132563609151173\n",
      "('annotation_10', 'annotation_12'): 0.48822296214083105\n",
      "('annotation_10', 'annotation_71'): 0.4135585268590194\n",
      "('annotation_12', 'annotation_71'): 0.29262844378257624\n",
      "\n",
      "\n",
      "Average Kappa Score:\n",
      "annotation_24: 0.4619801796445164\n",
      "annotation_10: 0.453757454758639\n",
      "annotation_12: 0.4353251444964594\n",
      "annotation_71: 0.3691708689110358\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_annotators = {}\n",
    "\n",
    "for k, v in reddit.items():\n",
    "    current_reddit = v.iloc[:, 1:]\n",
    "    \n",
    "    # get labels for each annotator\n",
    "    annotators = {}\n",
    "    for anno in current_reddit:\n",
    "        annotators[anno] = current_reddit[anno].values\n",
    "    \n",
    "    # get each pair of annotators -- 1,3 and 3,1 are same\n",
    "    # calculate agreement score\n",
    "    scores = {}\n",
    "    pairs = list(combinations(annotators, 2))\n",
    "    for pair in pairs:\n",
    "        scores[pair] = cohen_kappa_score(annotators[pair[0]], annotators[pair[1]])\n",
    "        \n",
    "    # claculate average kappa score\n",
    "    annotators_avg = {}\n",
    "    for k_, v_ in annotators.items():\n",
    "        #print(k_)\n",
    "        total = 0\n",
    "        for k__, v__ in scores.items():\n",
    "            if k_ in k__: # find pairs where current annotator is in -- 3 pairs\n",
    "                #print(k__)\n",
    "                total += scores[k__]\n",
    "        annotators_avg[k_] = total / 3\n",
    "    reddit_annotators[k] = annotators_avg\n",
    "        \n",
    "    print(k)\n",
    "    print(\"Cohen's kappa agreement score:\")\n",
    "    print('\\n'.join([str(k_) + \": \" + str(v_) for k_, v_ in scores.items()]))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Average Kappa Score:')\n",
    "    print('\\n'.join([str(k_) + \": \" + str(v_) for k_, v_ in annotators_avg.items()]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble dataset\n",
    "final_data = {'text': [], 'label': []}\n",
    "# for dupe texts -- \"[deleted]\", \"[removed]\", etc\n",
    "unique_texts = {}\n",
    "dupes = {}\n",
    "for k, v in reddit.items():\n",
    "    cur_data = v\n",
    "    annotators = reddit_annotators[k]\n",
    "    \n",
    "    # remove annotators with kappa < 0.2 -- unreliable\n",
    "    for anno in annotators:\n",
    "        if annotators[anno] < 0.2:\n",
    "            cur_data = cur_data.drop(columns = [anno])\n",
    "            #print('unreliable', anno)\n",
    "    \n",
    "    # get frequent labels for each text\n",
    "    frequent_labels = cur_data.iloc[:, 1:].mode(axis = 1)\n",
    "    #print(frequent_labels)\n",
    "    for i in range(len(cur_data)):\n",
    "        cur_text = cur_data.loc[i, 'text']\n",
    "        \n",
    "        freqs = {key: (0, 0) for key in frequent_labels.loc[i].tolist() if not pd.isnull(key)}\n",
    "        # key: (total kappa, # annotators)\n",
    "        # use freq label with higher reliability annotators\n",
    "        \n",
    "        # get kappas for the annotators that chose the most frequent for current text\n",
    "        for anno, cur_label in cur_data.iloc[i, 1:].iteritems():\n",
    "            #print('index: ', anno, 'value: ', cur_label)\n",
    "            if cur_label in freqs:\n",
    "                freqs[cur_label] = (freqs[cur_label][0] + annotators[anno], freqs[cur_label][1] + 1)\n",
    "        \n",
    "        # get best label based on reliability\n",
    "        max_avg_kappa = 0\n",
    "        best_label = 'None'\n",
    "        for cur_label, cur_v in freqs.items():\n",
    "            cur_avg_kappa = cur_v[0] / cur_v[1] if cur_v[1] else 0\n",
    "            if cur_avg_kappa > max_avg_kappa and cur_label != 'Missing': \n",
    "                # if missing ignore, so essentially use the next available highest kappa\n",
    "                max_avg_kappa = cur_avg_kappa\n",
    "                best_label = cur_label\n",
    "                \n",
    "        # if text is a duplicate, keep highest kappa\n",
    "        if cur_text in unique_texts:\n",
    "            if cur_text in dupes: # already ahve one duplicate -- if current lable higher kappa then change\n",
    "                if max_avg_kappa > dupes[cur_text][1]:\n",
    "                    dupes[cur_text] = (best_label, max_avg_kappa)\n",
    "            else: # new duplkicate, use last dupe (in unique texts) and choose higher kappa\n",
    "                if max_avg_kappa > unique_texts[cur_text][1]: # current label is best, so change it\n",
    "                    dupes[cur_text] = (best_label, max_avg_kappa)\n",
    "                else: # keep value in unique text\n",
    "                    dupes[cur_text] = (unique_texts[cur_text][0], unique_texts[cur_text][1])\n",
    "        else:\n",
    "            unique_texts[cur_text] = (best_label, max_avg_kappa)\n",
    "        \n",
    "        final_data['text'].append(cur_text)\n",
    "        final_data['label'].append(best_label)\n",
    "    \n",
    "final_dataset = pd.DataFrame(data = final_data)\n",
    "# remove duplicates and update the label with the highest kappa\n",
    "# 14 dupes with 2 uniques, so remove 12\n",
    "final_dataset = final_dataset.drop_duplicates(subset = ['text'])\n",
    "for k in dupes:\n",
    "    final_dataset.loc[final_dataset['text'] == k, 'label'] = dupes[k][0]\n",
    "\n",
    "# Remove None labels -- labels that were 'Missing' even after choosing the most frequent and avg kappa scores\n",
    "# no None labels tho\n",
    "FINAL = final_dataset[final_dataset['label'] != 'None']\n",
    "FINAL.to_csv('SECONDARY-Reddit_Stance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just give those people a wide berth, and do w...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would love to know as well. But then, daycar...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moral people will see it as an easy way to not...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>masking all time as i dont trust variants and ...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not just \"when out\", but when inside something...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>What vaccine. Why is it presumed that a vac wi...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Nope, you can still wear a mask if you want th...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>&gt; Too bad this was published after my district...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>It's simple either youreally care about the ki...</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>I see the reports, guys. As much as some of yo...</td>\n",
       "      <td>pro-mitigation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           label\n",
       "0     Just give those people a wide berth, and do w...  pro-mitigation\n",
       "1    I would love to know as well. But then, daycar...         unclear\n",
       "2    Moral people will see it as an easy way to not...  pro-mitigation\n",
       "3    masking all time as i dont trust variants and ...  pro-mitigation\n",
       "4    Not just \"when out\", but when inside something...  pro-mitigation\n",
       "..                                                 ...             ...\n",
       "295  What vaccine. Why is it presumed that a vac wi...         unclear\n",
       "296  Nope, you can still wear a mask if you want th...  pro-mitigation\n",
       "297  > Too bad this was published after my district...  pro-mitigation\n",
       "298  It's simple either youreally care about the ki...         unclear\n",
       "299  I see the reports, guys. As much as some of yo...  pro-mitigation\n",
       "\n",
       "[288 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
